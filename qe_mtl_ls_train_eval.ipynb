{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !pip install indic_nlp_library\n",
        "!pip install sentencepiece\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "-CGHLTIMIMCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22E1BYarZP0z"
      },
      "outputs": [],
      "source": [
        "# from indicnlp.tokenize.indic_tokenize import trivial_tokenize_indic\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModel\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "from torch.nn import CrossEntropyLoss, MSELoss\n",
        "from torch.optim import Adam\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EXP_DIR = 'en-de'\n",
        "SENT_TRAIN_FILE_PATH = 'train.ende.df.short.tsv'\n",
        "SENT_VAL_FILE_PATH = 'dev.ende.df.short.tsv'\n",
        "SENT_TEST_FILE_PATH = 'test20.ende.df.short.tsv'\n",
        "\n",
        "WORD_TRAIN_SRC_FILE_PATH = 'train.src'\n",
        "WORD_TRAIN_TGT_FILE_PATH = 'train.mt'\n",
        "WORD_TRAIN_SRC_TAGS_FILE_PATH = 'train.source_tags'\n",
        "WORD_TRAIN_TGT_TAGS_FILE_PATH = 'train.tags'\n",
        "\n",
        "WORD_VAL_SRC_FILE_PATH = 'dev.src'\n",
        "WORD_VAL_TGT_FILE_PATH = 'dev.mt'\n",
        "WORD_VAL_SRC_TAGS_FILE_PATH = 'dev.source_tags'\n",
        "WORD_VAL_TGT_TAGS_FILE_PATH = 'dev.tags'\n",
        "\n",
        "WORD_TEST_SRC_FILE_PATH = 'test20.src'\n",
        "WORD_TEST_TGT_FILE_PATH = 'test20.mt'\n",
        "WORD_TEST_SRC_TAGS_FILE_PATH = 'test20.source_tags'\n",
        "WORD_TEST_TGT_TAGS_FILE_PATH = 'test20.tags'\n",
        "\n",
        "MAX_LEN = 256\n",
        "SEP_TOKEN = '</s>'\n",
        "LABEL_ALL_TOKENS = False\n",
        "\n",
        "MODEL_TYPE = 'xlm-roberta-base'\n",
        "NUM_EPOCHS = 5\n",
        "NUM_ACC_STEPS = 1\n",
        "BATCH_SIZE = 16\n",
        "LR_RATE = 1e-5\n",
        "PATIENCE = 10\n",
        "MIN_DELTA = 5.\n",
        "\n",
        "if not os.path.exists(os.path.join(os.getcwd(), 'Outputs')):\n",
        "  os.mkdir('Outputs')\n",
        "BEST_MODEL_PATH = os.path.join(os.getcwd(), 'Outputs', EXP_DIR + '_' + 'best_model.pt')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "4BMraYZxCJhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_TYPE)"
      ],
      "metadata": {
        "id": "PPdjokCxCTR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(sent_file_path, word_src_path, word_tgt_path, word_src_tags_path, word_tgt_tags_path, sep_token):\n",
        "  lines = open(sent_file_path, 'r').readlines()[1:]\n",
        "  sent_labels = [float(line.strip().split('\\t')[-2]) for line in lines]\n",
        "\n",
        "  with open(word_src_path) as f1, open(word_tgt_path) as f2:\n",
        "    src_sents = [str(sent).strip() for sent in f1.readlines()]\n",
        "    tgt_sents = [str(sent).strip() for sent in f2.readlines()]\n",
        "\n",
        "  with open(word_src_tags_path) as f1, open(word_tgt_tags_path) as f2:\n",
        "    src_tags = [str(sent).strip() for sent in f1.readlines()]\n",
        "    tgt_tags = [str(sent).strip() for sent in f2.readlines()]\n",
        "\n",
        "  input_sents = []\n",
        "  for s, t in zip(src_sents, tgt_sents):\n",
        "    new_t = \"<gap> \" + \" <gap> \".join(t.split(\" \"))\n",
        "    input_sent = s + \" \" + sep_token + \" \" + sep_token + \" \" + new_t\n",
        "    input_sents.append(input_sent)\n",
        "\n",
        "  word_labels = []\n",
        "  for s, t in zip(src_tags, tgt_tags):\n",
        "    tag_seq = s.split(\" \") + [\"OK\"] + [\"OK\"] + t.split(\" \")[:-1]\n",
        "    word_labels.append(tag_seq)\n",
        "\n",
        "  return input_sents, word_labels, sent_labels"
      ],
      "metadata": {
        "id": "a4B9C6bzBEKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_inputs, train_word_labels, train_sent_labels = load_data(SENT_TRAIN_FILE_PATH, WORD_TRAIN_SRC_FILE_PATH, WORD_TRAIN_TGT_FILE_PATH, WORD_TRAIN_SRC_TAGS_FILE_PATH, WORD_TRAIN_TGT_TAGS_FILE_PATH, SEP_TOKEN)\n",
        "val_inputs, val_word_labels, val_sent_labels = load_data(SENT_VAL_FILE_PATH, WORD_VAL_SRC_FILE_PATH, WORD_VAL_TGT_FILE_PATH, WORD_VAL_SRC_TAGS_FILE_PATH, WORD_VAL_TGT_TAGS_FILE_PATH, SEP_TOKEN)\n",
        "test_inputs, test_word_labels, test_sent_labels = load_data(SENT_TEST_FILE_PATH, WORD_TEST_SRC_FILE_PATH, WORD_TEST_TGT_FILE_PATH, WORD_TEST_SRC_TAGS_FILE_PATH, WORD_TEST_TGT_TAGS_FILE_PATH, SEP_TOKEN)\n",
        "\n",
        "train_df = pd.DataFrame({\"text\": train_inputs, \"word_labels\": train_word_labels, \"sent_labels\": train_sent_labels})\n",
        "val_df = pd.DataFrame({\"text\": val_inputs, \"word_labels\": val_word_labels, \"sent_labels\": val_sent_labels})\n",
        "test_df = pd.DataFrame({\"text\": test_inputs, \"word_labels\": test_word_labels, \"sent_labels\": test_sent_labels})"
      ],
      "metadata": {
        "id": "5Xc80jErXZIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_to_ids = {\"OK\": 0, \"BAD\": 1}\n",
        "ids_to_labels = {0: \"OK\", 1: \"BAD\"}"
      ],
      "metadata": {
        "id": "z75sv7hQ3Wek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def align_label(text, labels, label_all_tokens):\n",
        "  tokenized_input = tokenizer(text, padding='max_length', max_length=MAX_LEN, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "  word_ids = tokenized_input.word_ids()\n",
        "\n",
        "  previous_word_idx = None\n",
        "  label_ids = []\n",
        "  for word_idx in word_ids:\n",
        "\n",
        "      if word_idx is None:\n",
        "          label_ids.append(-100)\n",
        "\n",
        "      elif word_idx != previous_word_idx:\n",
        "          try:\n",
        "              label_ids.append(labels_to_ids[labels[word_idx]])\n",
        "          except:\n",
        "              label_ids.append(-100)\n",
        "      else:\n",
        "          try:\n",
        "              label_ids.append(labels_to_ids[labels[word_idx]] if label_all_tokens else -100)\n",
        "          except:\n",
        "              label_ids.append(-100)\n",
        "      previous_word_idx = word_idx\n",
        "\n",
        "  return label_ids"
      ],
      "metadata": {
        "id": "OJrGgDo6_FIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataSequence(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, df):\n",
        "\n",
        "        word_lb = df['word_labels'].values.tolist()\n",
        "        sent_lb = df['sent_labels'].values.tolist()\n",
        "        txts = df['text'].values.tolist()\n",
        "        self.texts = [tokenizer(str(txt),\n",
        "                               padding='max_length', max_length = MAX_LEN, truncation=True, return_tensors=\"pt\") for txt in txts]\n",
        "        self.word_labels = [align_label(t, l, LABEL_ALL_TOKENS) for t, l in zip(txts, word_lb)]\n",
        "        self.sent_labels = sent_lb\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return len(self.sent_labels)\n",
        "\n",
        "    def get_batch_data(self, idx):\n",
        "\n",
        "        return self.texts[idx]\n",
        "\n",
        "    def get_batch_word_labels(self, idx):\n",
        "\n",
        "        return torch.LongTensor(self.word_labels[idx])\n",
        "\n",
        "    def get_batch_sent_labels(self, idx):\n",
        "\n",
        "        return torch.tensor(self.sent_labels[idx], dtype=torch.float)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        batch_data = self.get_batch_data(idx)\n",
        "        batch_word_labels = self.get_batch_word_labels(idx)\n",
        "        batch_sent_labels = self.get_batch_sent_labels(idx)\n",
        "\n",
        "        return batch_data, batch_word_labels, batch_sent_labels"
      ],
      "metadata": {
        "id": "gMx2ogLXIxeE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = DataSequence(train_df)\n",
        "val_dataset = DataSequence(val_df)\n",
        "test_dataset = DataSequence(test_df)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, num_workers=2, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, num_workers=2, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, num_workers=2, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "metadata": {
        "id": "4wRFyYSbqZGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MTLModel(torch.nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "\n",
        "    self.num_labels = config.num_labels\n",
        "\n",
        "    self.base_model = AutoModel.from_pretrained(MODEL_TYPE, config=config, add_pooling_layer=False)\n",
        "    classifier_dropout = (\n",
        "            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n",
        "        )\n",
        "    # word layers\n",
        "    self.dropout = nn.Dropout(classifier_dropout)\n",
        "    self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
        "\n",
        "    # sentence layers\n",
        "    self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "    self.out_proj = nn.Linear(config.hidden_size, 1)\n",
        "\n",
        "  def forward(self, input_ids, attn_mask, word_labels=None, sent_labels=None):\n",
        "    outputs = self.base_model(input_ids, attn_mask)\n",
        "    sequence_output = outputs[0]\n",
        "\n",
        "    # token_classification\n",
        "    sequence_op_dropout = self.dropout(sequence_output)\n",
        "    logits = self.classifier(sequence_op_dropout)\n",
        "\n",
        "    # regression\n",
        "    x = sequence_output[:, 0, :]\n",
        "    x = self.dropout(x)\n",
        "    x = self.dense(x)\n",
        "    x = torch.tanh(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.out_proj(x)\n",
        "\n",
        "    if word_labels is not None and sent_labels is not None:\n",
        "      word_loss_fn = CrossEntropyLoss()\n",
        "      word_loss = word_loss_fn(logits.view(-1, self.num_labels), word_labels.view(-1))\n",
        "\n",
        "      sent_loss_fn = MSELoss()\n",
        "      sent_loss = sent_loss_fn(x.squeeze(), sent_labels.squeeze())\n",
        "\n",
        "      return logits, x, sent_loss, word_loss\n",
        "\n",
        "    else:\n",
        "      return logits, x"
      ],
      "metadata": {
        "id": "QSL850CYrdnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = AutoConfig.from_pretrained(MODEL_TYPE)\n",
        "config.num_labels = len(labels_to_ids)\n",
        "\n",
        "model = MTLModel(config)\n",
        "model.to(device)\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=LR_RATE)"
      ],
      "metadata": {
        "id": "Mf3Ak0MCP9Df"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sum(dict((p.data_ptr(), p.numel()) for p in model.parameters()).values())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sD9Vpthmr3qE",
        "outputId": "6af705df-6ea5-4b3d-eb52-974b2a8044be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "278045955"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model, dataloader):\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  total_val_loss, total_val_mse, total_val_acc = np.inf, np.inf, 0.\n",
        "  for data, word_labels, sent_labels in dataloader:\n",
        "\n",
        "    input_ids = data['input_ids'].squeeze(1).to(device)\n",
        "    attn_masks = data['attention_mask'].squeeze(1).to(device)\n",
        "    word_labels = word_labels.to(device)\n",
        "    sent_labels = sent_labels.to(device)\n",
        "\n",
        "    logits, x, sent_loss, word_loss = model(input_ids, attn_masks, word_labels, sent_labels)\n",
        "    loss = torch.add(sent_loss, word_loss)\n",
        "\n",
        "    # word predictions\n",
        "    for i in range(logits.shape[0]):\n",
        "      word_logits_clean = logits[i][word_labels[i] != -100]\n",
        "      word_label_clean = word_labels[i][word_labels[i] != -100]\n",
        "      predictions = word_logits_clean.argmax(dim=1)\n",
        "      acc = (predictions == word_label_clean).float().mean()\n",
        "      total_val_acc += acc\n",
        "\n",
        "    total_val_mse += sent_loss.item()\n",
        "    total_val_loss += loss.item()\n",
        "\n",
        "  return total_val_loss, total_val_mse, total_val_acc"
      ],
      "metadata": {
        "id": "FwG6Muvf1Yni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping():\n",
        "    def __init__(self, patience=1, min_delta=0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.min_validation_loss = np.inf\n",
        "\n",
        "    def early_stopping(self, validation_loss):\n",
        "        if validation_loss < self.min_validation_loss:\n",
        "            self.min_validation_loss = validation_loss\n",
        "            self.counter = 0\n",
        "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "early_stopping = EarlyStopping(patience=3, min_delta=10)"
      ],
      "metadata": {
        "id": "JtfYnayuGG1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_val_loss = np.inf\n",
        "train_losses, val_losses= [], []\n",
        "train_accs, val_accs = [], []\n",
        "train_mses, val_mses = [], []\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  total_train_acc, total_train_mse = 0., np.inf\n",
        "  total_train_loss, total_dev_loss = np.inf, np.inf\n",
        "  for data, word_labels, sent_labels in tqdm(train_dataloader):\n",
        "\n",
        "    input_ids = data['input_ids'].squeeze(1).to(device)\n",
        "    attn_masks = data['attention_mask'].squeeze(1).to(device)\n",
        "    word_labels = word_labels.to(device)\n",
        "    sent_labels = sent_labels.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    logits, x, sent_loss, word_loss = model(input_ids, attn_masks, word_labels, sent_labels)\n",
        "\n",
        "    loss = torch.add(sent_loss, word_loss)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      # word predictions\n",
        "      for i in range(logits.shape[0]):\n",
        "        word_logits_clean = logits[i][word_labels[i] != -100]\n",
        "        word_label_clean = word_labels[i][word_labels[i] != -100]\n",
        "        predictions = word_logits_clean.argmax(dim=1)\n",
        "        acc = (predictions == word_label_clean).float().mean()\n",
        "\n",
        "        total_train_acc += acc\n",
        "\n",
        "      total_train_mse += sent_loss.item()\n",
        "      total_train_loss += loss.item()\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  total_val_loss, total_val_mse, total_val_acc = validate(model, val_dataloader)\n",
        "\n",
        "  if best_val_loss >= total_val_loss:\n",
        "    best_val_loss = total_val_loss\n",
        "    torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
        "\n",
        "  if early_stopping.early_stopping(total_val_loss):\n",
        "    break\n",
        "\n",
        "  train_accs.append(total_train_acc)\n",
        "  train_mses.append(total_train_mse)\n",
        "  train_losses.append(total_train_loss)\n",
        "  val_accs.append(total_val_acc)\n",
        "  val_mses.append(total_val_mse)\n",
        "  val_losses.append(total_val_loss)\n",
        "\n",
        "  print(f'Epochs: {epoch + 1} | Train_Loss: {total_train_loss / len(train_df): .3f} | Train_Accuracy: {total_train_acc / len(train_df): .3f} | Train_MSE: {total_train_mse: .3} | Val_Loss: {total_val_loss / len(val_df): .3f} | Val_Accuracy: {total_val_acc/ len(val_df): .3f} | Val_MSE: {total_val_mse: .3f}')"
      ],
      "metadata": {
        "id": "lRItICAxOpCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.eval()\n",
        "\n",
        "# for data, word_labels, sent_labels in tqdm(test_dataloader):\n",
        "\n",
        "#     input_ids = data['input_ids'].squeeze(1).to(device)\n",
        "#     attn_masks = data['attention_mask'].squeeze(1).to(device)\n",
        "#     word_labels = word_labels.to(device)\n",
        "#     sent_labels = sent_labels.to(device)\n",
        "\n",
        "#     logits, x = model(input_ids, attn_masks)\n",
        "\n",
        "#     for i in range(logits.shape[0]):\n",
        "#         word_logits_clean = logits[i][word_labels[i] != -100]\n",
        "#         word_label_clean = word_labels[i][word_labels[i] != -100]\n",
        "#         predictions = word_logits_clean.argmax(dim=1)\n",
        "#         acc = (predictions == word_label_clean).float().mean()"
      ],
      "metadata": {
        "id": "D_gfBbOAgkRs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}